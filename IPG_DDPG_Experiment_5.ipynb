{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1xgF98QyT6L",
    "outputId": "054e88c0-e2dd-466e-af88-9fc573fe72a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "18Hsu9QacC1v"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hToyQSrUyg0e",
    "outputId": "ce75c11c-469c-4b55-a833-08afd38c04e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hGtnHeMDymG6"
   },
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k2O1mY16Wz6n"
   },
   "outputs": [],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "env = env\n",
    "SEASONS = 1000\n",
    "episode = 0\n",
    "replay_count = 0\n",
    "success_value = 70\n",
    "lr_a = 0.0002\n",
    "lr_c = 0.0002\n",
    "epochs = 10\n",
    "training_batch = 1024\n",
    "batch_size = 128\n",
    "epsilon = 0.2\n",
    "gamma = 0.993\n",
    "lmbda = 0.7\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "critic_optimizer = tf.keras.optimizers.Adam(lr_a)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(lr_c)\n",
    "baseline_optimizer = tf.keras.optimizers.Adam(lr_c)\n",
    "total_episodes = 100\n",
    "gamma = 0.99\n",
    "tau = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4OSy8qj2zylK"
   },
   "outputs": [],
   "source": [
    "def get_actor(env):\n",
    "  state_size = env.observation_space.shape\n",
    "  action_size = env.action_space.shape[0]\n",
    "  # Initialize weights between -3e-3 and 3-e3\n",
    "  last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape = state_size)\n",
    "  out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "  out = layers.Dense(256, activation=\"relu\")(out)\n",
    "  outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "  # Our upper bound is 2.0 for Pendulum.\n",
    "  outputs = outputs * upper_bound\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ErWaVoKP2JAH"
   },
   "outputs": [],
   "source": [
    "def train_actor(states, advantages, actions, actor, epsilon, action_size, optimizer, old_pi, critic, b, s_batch):\n",
    "  with tf.GradientTape() as tape:\n",
    "    mean = tf.squeeze(actor(states))\n",
    "\n",
    "    # On-policy ppo loss\n",
    "    logstd = tf.Variable(np.zeros(shape=env.action_space.shape[0], dtype=np.float32))\n",
    "    std = tf.squeeze(tf.exp(logstd))\n",
    "    new_pi = tfp.distributions.Normal(mean, std)\n",
    "\n",
    "    ratio = tf.exp(new_pi.log_prob(tf.squeeze(actions)) -\n",
    "                      old_pi.log_prob(tf.squeeze(actions)))\n",
    "\n",
    "    adv_stack = tf.stack([advantages for _ in range(action_size)], axis=1)\n",
    "    p1 = ratio * adv_stack\n",
    "    p2 = tf.clip_by_value(ratio, 1. - epsilon, 1. + epsilon) * adv_stack\n",
    "    ppo_loss = K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    # Off-policy loss\n",
    "    mean_off = actor(s_batch)\n",
    "    q_values = critic([s_batch, mean_off])\n",
    "    sum_q_values = K.sum(K.mean(q_values))\n",
    "    off_loss = ((b / len(s_batch)) * sum_q_values)\n",
    "\n",
    "    actor_loss = -tf.reduce_sum(ppo_loss + off_loss)\n",
    "    # actor_loss = -ppo_loss\n",
    "    actor_weights = actor.trainable_variables\n",
    "\n",
    "    # outside gradient tape\n",
    "    actor_grad = tape.gradient(actor_loss, actor_weights)\n",
    "    optimizer.apply_gradients(zip(actor_grad, actor_weights))\n",
    "\n",
    "    return actor_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rA85MAafz7Tk"
   },
   "outputs": [],
   "source": [
    "def get_critic(env):\n",
    "  state_size = env.observation_space.shape\n",
    "  action_size = env.action_space.shape[0]\n",
    "  upper_bound = env.action_space.high[0]\n",
    "  lr = 0.0002 \n",
    "  gamma = 0.993\n",
    "  train_step_count = 0\n",
    "  optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "  state_input = layers.Input(shape=state_size)\n",
    "  # State as input\n",
    "  state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "  state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "  # Action as input\n",
    "  action_input = layers.Input(shape=(action_size))\n",
    "  action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "  # Both are passed through seperate layer before concatenating\n",
    "  concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "  out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "  out = layers.Dense(256, activation=\"relu\")(out)\n",
    "  outputs = layers.Dense(1)(out)\n",
    "\n",
    "  # Outputs single value for give state-action\n",
    "  model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5j1DFRVTG00q"
   },
   "outputs": [],
   "source": [
    "def train_critic(state_batch, action_batch, critic, optimizer, y):\n",
    "  # train_step_count += 1\n",
    "  with tf.GradientTape() as tape:\n",
    "    critic_weights = critic.trainable_variables\n",
    "    critic_value = critic([state_batch, action_batch])\n",
    "    critic_loss = tf.math.reduce_mean(tf.square(y - critic_value))\n",
    "\n",
    "    critic_grad = tape.gradient(critic_loss, critic_weights)\n",
    "    optimizer.apply_gradients(zip(critic_grad, critic_weights))\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9-bwsRgQz_NZ"
   },
   "outputs": [],
   "source": [
    "def get_baseline(env):\n",
    "  state_size = env.observation_space.shape\n",
    "  action_size = env.action_space.shape[0]\n",
    "\n",
    "  lr = 0.0002\n",
    "  optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "  state_input = tf.keras.layers.Input(shape=state_size)\n",
    "  f = tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)(state_input)\n",
    "\n",
    "  out = tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)(f)\n",
    "  out = tf.keras.layers.Dense(64, activation=\"relu\", trainable=True)(out)\n",
    "  out = tf.keras.layers.Dense(32, activation=\"relu\", trainable=True)(out)\n",
    "  net_out = tf.keras.layers.Dense(1, trainable=True)(out)\n",
    "\n",
    "  # Outputs single value for a given state = V(s)\n",
    "  model = tf.keras.Model(inputs=state_input, outputs=net_out)\n",
    "       \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "htE8EGoV5kQF"
   },
   "outputs": [],
   "source": [
    "def train_baseline(states, baseline, optimizer, returns):\n",
    "  with tf.GradientTape() as tape:\n",
    "      critic_weights = baseline.trainable_variables\n",
    "      critic_values = baseline(states)\n",
    "      critic_loss = tf.math.reduce_mean(tf.square(returns - critic_values))\n",
    "\n",
    "  critic_grad = tape.gradient(critic_loss, critic_weights)\n",
    "  optimizer.apply_gradients(zip(critic_grad, critic_weights))\n",
    "  return critic_loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_lq1sVbp0Cjh"
   },
   "outputs": [],
   "source": [
    "def policy(actor, state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor(state))\n",
    "    noise = noise_object()\n",
    "\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_p0pD35-0IWd"
   },
   "outputs": [],
   "source": [
    "def compute_advantages(baseline, r_batch, s_batch, ns_batch, d_batch):\n",
    "  gamma = 0.99\n",
    "  lmbda = 0.7\n",
    "\n",
    "  s_values = tf.squeeze(baseline(s_batch))  # input: tensor\n",
    "  ns_values = tf.squeeze(baseline(ns_batch))\n",
    "  returns = []\n",
    "  gae = 0  # generalized advantage estimate\n",
    "  for i in reversed(range(len(r_batch))):\n",
    "      delta = r_batch[i] + gamma * ns_values[i] * (1 - d_batch[i]) - s_values[i]\n",
    "      gae = delta + gamma * lmbda * (1 - d_batch[i]) * gae\n",
    "      returns.insert(0, gae + s_values[i])\n",
    "\n",
    "  returns = np.array(returns)\n",
    "  adv = returns - s_values.numpy()  # Q - V\n",
    "  adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)  # output: numpy array\n",
    "  return adv, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5F6fc02Q0Mug"
   },
   "outputs": [],
   "source": [
    "def compute_targets(actor, critic, r_batch, ns_batch, d_batch):\n",
    "  gamma = np.float32(0.99)\n",
    "  mean = actor(ns_batch)\n",
    "\n",
    "  target_critic_1 = critic([ns_batch, mean])\n",
    "  # print(\"tc1\", type(target_critic_1))\n",
    "  # print(\"r_batch\", type(r_batch))\n",
    "  # print(\"gamma\", type(gamma))\n",
    "  # print(\"d_batch\", type(d_batch))\n",
    "  r_batch = tf.cast(r_batch, dtype=tf.float32)\n",
    "  d_batch = tf.cast(d_batch, dtype=tf.float32)\n",
    "  target_critic_1 = tf.cast(target_critic_1, dtype=tf.float32)\n",
    "\n",
    "  # y = r_batch + (1 - d_batch) * target_critic_1\n",
    "  y = r_batch + gamma * (1 - d_batch) * target_critic_1\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ChDdaICZ0QlJ"
   },
   "outputs": [],
   "source": [
    "def compute_adv_bar(actor, critic, s_batch, a_batch):\n",
    "  mean = actor(s_batch)\n",
    "  x = tf.squeeze(a_batch) - tf.squeeze(mean)\n",
    "  y = tf.squeeze(critic([s_batch, mean]))\n",
    "  adv_bar = y * x\n",
    "\n",
    "  return adv_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "W9K_xRXR0VSH"
   },
   "outputs": [],
   "source": [
    "def replay(actor, critic, baseline, states, actions, rewards, dones, next_states):\n",
    "\n",
    "  a_loss_list = []\n",
    "  c_loss_list = []\n",
    "\n",
    "  n_split = len(rewards)\n",
    "\n",
    "  states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "  actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "  rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "  dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "  next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "  advantages, returns = compute_advantages(baseline, rewards, states, next_states, dones)\n",
    "\n",
    "  b = 0\n",
    "  use_CV = False\n",
    "  v = 0.2\n",
    "  if use_CV:\n",
    "      # Compute critic-based advantage\n",
    "      adv_bar = compute_adv_bar(states, actions)\n",
    "      ls = advantages - adv_bar\n",
    "      b = 1\n",
    "  else:\n",
    "      ls = advantages\n",
    "      b = v\n",
    "\n",
    "  ls *= (1 - v)\n",
    "\n",
    "  s_split = tf.split(states, n_split)\n",
    "  a_split = tf.split(actions, n_split)\n",
    "  t_split = tf.split(returns, n_split)\n",
    "  adv_split = tf.split(advantages, n_split)\n",
    "  ls_split = tf.split(ls, n_split)\n",
    "  indexes = np.arange(n_split, dtype=int)\n",
    "\n",
    "  # current policy\n",
    "  # mean, std = actor(states)\n",
    "  mean = tf.squeeze(actor(states))\n",
    "  # std = tf.squeeze(tf.exp(actor.logstd))\n",
    "  logstd = tf.Variable(np.zeros(shape=env.action_space.shape[0], dtype=np.float32))\n",
    "  std = tf.squeeze(tf.exp(logstd))\n",
    "  pi = tfp.distributions.Normal(mean, std)\n",
    "\n",
    "  a_loss, c_loss = 0, 0\n",
    "\n",
    "  np.random.shuffle(indexes)\n",
    "  for _ in range(epochs):\n",
    "      s_batch, a_batch, r_batch, ns_batch, d_batch = buffer.sample()\n",
    "      for i in indexes:\n",
    "          old_pi = pi[i * batch_size: (i + 1) * batch_size]\n",
    "          # Update actor\n",
    "          a_loss = train_actor(s_split[i], ls_split[i], a_split[i], actor, 0.2, num_actions, actor_optimizer, old_pi, critic, b, s_batch)\n",
    "          a_loss_list.append(a_loss)\n",
    "          # Update baseline\n",
    "          v_loss = train_baseline(s_split[i], baseline, baseline_optimizer, t_split[i])\n",
    "\n",
    "      # Update critic\n",
    "      y = compute_targets(actor, critic, r_batch, ns_batch, d_batch)\n",
    "      c_loss = train_critic(s_batch, a_batch, critic, critic_optimizer, y)\n",
    "\n",
    "      c_loss_list.append(c_loss)\n",
    "\n",
    "      # replay_count += 1\n",
    "\n",
    "      return np.mean(a_loss_list), np.mean(c_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "umrPyfIQ6UJn"
   },
   "outputs": [],
   "source": [
    "# Validation Routine\n",
    "def validate(self, env, max_eps=50):\n",
    "  ep_reward_list = []\n",
    "  for ep in range(max_eps):\n",
    "      if self.is_mujo:\n",
    "          state = env.reset()[\"observation\"]\n",
    "      else:\n",
    "          state = env.reset()\n",
    "          state = np.asarray(state, dtype=np.float32)\n",
    "      t = 0\n",
    "      ep_reward = 0\n",
    "      while True:\n",
    "          action = policy(actor, tf_prev_state, ou_noise)\n",
    "          next_obsv, reward, done, _ = env.step(action)\n",
    "          next_state = np.asarray(next_obsv, dtype=np.float32)\n",
    "          state = next_state\n",
    "          ep_reward += reward\n",
    "          t += 1\n",
    "          if done:\n",
    "              ep_reward_list.append(ep_reward)\n",
    "              break\n",
    "\n",
    "  mean_ep_reward = np.mean(ep_reward_list)\n",
    "  return mean_ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vTxDPBzAyylu"
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, actor, critic, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.done_batch = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_batch[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor(state_batch, training=True)\n",
    "            critic_value = critic([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "     # We compute the loss and update parameters\n",
    "    def sample(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        # reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_batch[batch_indices])\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "w3pDYuEczOzz"
   },
   "outputs": [],
   "source": [
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c23L5DUO57Oo"
   },
   "outputs": [],
   "source": [
    "def save_model(self, path, actor_filename, critic_filename, baseline_filename):\n",
    "  actor_file = path + actor_filename\n",
    "  critic_file = path + critic_filename\n",
    "  baseline_file = path + baseline_filename\n",
    "  self.actor.save_weights(actor_file)\n",
    "  self.critic.save_weights(critic_file)\n",
    "  self.baseline.save_weights(baseline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8mw6Z3oZ5-KQ"
   },
   "outputs": [],
   "source": [
    "def load_model(self, path, actor_filename, critic_filename, baseline_filename):\n",
    "  actor_file = path + actor_filename\n",
    "  critic_file = path + critic_filename\n",
    "  baseline_file = path + baseline_filename\n",
    "  self.actor.model.load_weights(actor_file)\n",
    "  self.critic.model.load_weights(critic_file)\n",
    "  self.baseline.model.load_weights(baseline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0CQLwbL409zW"
   },
   "outputs": [],
   "source": [
    "# problem = \"Pendulum-v1\"\n",
    "# env = gym.make(problem)\n",
    "\n",
    "# env = env\n",
    "# SEASONS = 1000\n",
    "# episode = 0\n",
    "# replay_count = 0\n",
    "# success_value = 70\n",
    "# lr_a = 0.0002\n",
    "# lr_c = 0.0002\n",
    "# epochs = 10\n",
    "# training_batch = 1024\n",
    "# batch_size = 128\n",
    "# epsilon = 0.2\n",
    "# gamma = 0.993\n",
    "# lmbda = 0.7\n",
    "# std_dev = 0.2\n",
    "# ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "# critic_optimizer = tf.keras.optimizers.Adam(lr_a)\n",
    "# actor_optimizer = tf.keras.optimizers.Adam(lr_c)\n",
    "# baseline_optimizer = tf.keras.optimizers.Adam(lr_c)\n",
    "# total_episodes = 100\n",
    "# gamma = 0.99\n",
    "# tau = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZMF30pcu1Bzx"
   },
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "# buffer = Buffer(100000, batch_size)\n",
    "\n",
    "actor = get_actor(env)\n",
    "critic = get_critic(env)\n",
    "baseline = get_baseline(env)\n",
    "\n",
    "target_actor = get_actor(env)\n",
    "target_critic = get_critic(env)\n",
    "\n",
    "target_actor.set_weights(actor.get_weights())\n",
    "target_critic.set_weights(critic.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ecrXSlVAAVyO",
    "outputId": "b7ef9009-526f-40d3-c2b3-aff73da2bedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1328.9811737810319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 10 calls to <function _BaseOptimizer._update_step_xla at 0x7f4e5127cdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function _BaseOptimizer._update_step_xla at 0x7f4e5127cdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 1 * Avg Reward is ==> -1175.1682756628693\n",
      "Episode * 2 * Avg Reward is ==> -1359.068982887871\n",
      "Episode * 3 * Avg Reward is ==> -1546.6554320671344\n",
      "Episode * 4 * Avg Reward is ==> -1697.0428296429066\n",
      "Episode * 5 * Avg Reward is ==> -917.1907754257281\n",
      "Episode * 6 * Avg Reward is ==> -1434.7116365112336\n",
      "Episode * 7 * Avg Reward is ==> -862.0189332808401\n",
      "Episode * 8 * Avg Reward is ==> -1090.3083844380683\n",
      "Episode * 9 * Avg Reward is ==> -1332.5279318748114\n",
      "Episode * 10 * Avg Reward is ==> -984.136475655132\n",
      "Episode * 11 * Avg Reward is ==> -1198.617313620107\n",
      "Episode * 12 * Avg Reward is ==> -1320.8878377635706\n",
      "Episode * 13 * Avg Reward is ==> -1491.9229450850562\n",
      "Episode * 14 * Avg Reward is ==> -1536.7837032604075\n",
      "Episode * 15 * Avg Reward is ==> -1400.4965534808948\n",
      "Episode * 16 * Avg Reward is ==> -1701.2147949286912\n",
      "Episode * 17 * Avg Reward is ==> -1328.6291409443481\n",
      "Episode * 18 * Avg Reward is ==> -1472.0365052086775\n",
      "Episode * 19 * Avg Reward is ==> -1297.8815528855234\n",
      "Episode * 20 * Avg Reward is ==> -1030.7150310697602\n",
      "Episode * 21 * Avg Reward is ==> -1305.657335554139\n",
      "Episode * 22 * Avg Reward is ==> -1477.232718841816\n",
      "Episode * 23 * Avg Reward is ==> -949.449614666311\n",
      "Episode * 24 * Avg Reward is ==> -1015.4566438037442\n",
      "Episode * 25 * Avg Reward is ==> -1238.3385244113517\n",
      "Episode * 26 * Avg Reward is ==> -1230.23840876683\n",
      "Episode * 27 * Avg Reward is ==> -1509.486707627455\n",
      "Episode * 28 * Avg Reward is ==> -1177.727959236121\n",
      "Episode * 29 * Avg Reward is ==> -854.9975296402002\n",
      "Episode * 30 * Avg Reward is ==> -1092.8559897430046\n",
      "Episode * 31 * Avg Reward is ==> -1036.9179756079034\n",
      "Episode * 32 * Avg Reward is ==> -1337.9609219384824\n",
      "Episode * 33 * Avg Reward is ==> -1055.1751579364843\n",
      "Episode * 34 * Avg Reward is ==> -1220.6426050413188\n",
      "Episode * 35 * Avg Reward is ==> -1360.0846935279492\n",
      "Episode * 36 * Avg Reward is ==> -883.0143691303998\n",
      "Episode * 37 * Avg Reward is ==> -1226.5689890123147\n",
      "Episode * 38 * Avg Reward is ==> -1280.0495127256877\n",
      "Episode * 39 * Avg Reward is ==> -888.5948712048328\n",
      "Episode * 40 * Avg Reward is ==> -920.5001069817848\n",
      "Episode * 41 * Avg Reward is ==> -1137.6042626689414\n",
      "Episode * 42 * Avg Reward is ==> -1213.9694576178144\n",
      "Episode * 43 * Avg Reward is ==> -1243.9237239232473\n",
      "Episode * 44 * Avg Reward is ==> -1351.2025418927867\n",
      "Episode * 45 * Avg Reward is ==> -1284.2011786170685\n",
      "Episode * 46 * Avg Reward is ==> -1199.948736337253\n",
      "Episode * 47 * Avg Reward is ==> -1172.227275921737\n",
      "Episode * 48 * Avg Reward is ==> -1151.8690865714968\n",
      "Episode * 49 * Avg Reward is ==> -1308.1235137970734\n",
      "Episode * 50 * Avg Reward is ==> -1352.686699317531\n",
      "Episode * 51 * Avg Reward is ==> -1306.8855197391756\n",
      "Episode * 52 * Avg Reward is ==> -1215.4600971019142\n",
      "Episode * 53 * Avg Reward is ==> -1353.0960254048102\n",
      "Episode * 54 * Avg Reward is ==> -1353.9561797797157\n",
      "Episode * 55 * Avg Reward is ==> -1695.792944512011\n",
      "Episode * 56 * Avg Reward is ==> -1433.4801946661319\n",
      "Episode * 57 * Avg Reward is ==> -1657.1940625724717\n",
      "Episode * 58 * Avg Reward is ==> -1378.1070190939033\n",
      "Episode * 59 * Avg Reward is ==> -1311.0898949071561\n",
      "Episode * 60 * Avg Reward is ==> -1326.083287203245\n",
      "Episode * 61 * Avg Reward is ==> -1612.533077363964\n",
      "Episode * 62 * Avg Reward is ==> -1706.8085104583708\n",
      "Episode * 63 * Avg Reward is ==> -1319.5951336297542\n",
      "Episode * 64 * Avg Reward is ==> -1698.4855391351339\n",
      "Episode * 65 * Avg Reward is ==> -1409.71824026957\n",
      "Episode * 66 * Avg Reward is ==> -1399.0775007275818\n",
      "Episode * 67 * Avg Reward is ==> -1418.357190750396\n",
      "Episode * 68 * Avg Reward is ==> -1622.139769030599\n",
      "Episode * 69 * Avg Reward is ==> -1738.7704924094403\n",
      "Episode * 70 * Avg Reward is ==> -1197.4664953248828\n",
      "Episode * 71 * Avg Reward is ==> -811.340052037808\n",
      "Episode * 72 * Avg Reward is ==> -1227.0693298384242\n",
      "Episode * 73 * Avg Reward is ==> -902.981840993576\n",
      "Episode * 74 * Avg Reward is ==> -1059.3822831982275\n",
      "Episode * 75 * Avg Reward is ==> -1029.7880878517185\n",
      "Episode * 76 * Avg Reward is ==> -1223.347279227351\n",
      "Episode * 77 * Avg Reward is ==> -1412.1659479638574\n",
      "Episode * 78 * Avg Reward is ==> -1401.922590016793\n",
      "Episode * 79 * Avg Reward is ==> -1441.1474888589987\n",
      "Episode * 80 * Avg Reward is ==> -1192.1470479683255\n",
      "Episode * 81 * Avg Reward is ==> -1470.8439765855771\n",
      "Episode * 82 * Avg Reward is ==> -1237.0604755914053\n",
      "Episode * 83 * Avg Reward is ==> -1393.8769386321567\n",
      "Episode * 84 * Avg Reward is ==> -1396.5416942297502\n",
      "Episode * 85 * Avg Reward is ==> -1380.0919630309454\n",
      "Episode * 86 * Avg Reward is ==> -1135.2709120330153\n",
      "Episode * 87 * Avg Reward is ==> -991.54569038442\n",
      "Episode * 88 * Avg Reward is ==> -1462.9877363715036\n",
      "Episode * 89 * Avg Reward is ==> -1392.7234571648\n",
      "Episode * 90 * Avg Reward is ==> -1396.3199769613416\n",
      "Episode * 91 * Avg Reward is ==> -1331.599489397521\n",
      "Episode * 92 * Avg Reward is ==> -1340.875382674353\n",
      "Episode * 93 * Avg Reward is ==> -1358.2464962469471\n",
      "Episode * 94 * Avg Reward is ==> -1122.8068744875209\n",
      "Episode * 95 * Avg Reward is ==> -1490.6009792254479\n",
      "Episode * 96 * Avg Reward is ==> -1444.5712271236723\n",
      "Episode * 97 * Avg Reward is ==> -1243.3679654065515\n",
      "Episode * 98 * Avg Reward is ==> -1478.182075440795\n",
      "Episode * 99 * Avg Reward is ==> -1495.3511379670238\n",
      "Episode * 100 * Avg Reward is ==> -1075.067186926039\n",
      "Episode * 101 * Avg Reward is ==> -1349.6793043358143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cedeff4ace16>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mavg_reward_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0ma_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# After season\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-5779ff2bf881>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(actor, critic, baseline, states, actions, rewards, dones, next_states)\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0mold_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0;31m# Update actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m           \u001b[0ma_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m           \u001b[0ma_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0;31m# Update baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-006c90a63528>\u001b[0m in \u001b[0;36mtrain_actor\u001b[0;34m(states, advantages, actions, actor, epsilon, action_size, optimizer, old_pi, critic, b, s_batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Off-policy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmean_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_off\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msum_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 ):\n\u001b[0;32m-> 1145\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \"\"\"\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m                 \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autographed_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0m\u001b[1;32m   1124\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 object_name=(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36minject_argument_info_in_traceback\u001b[0;34m(fn, object_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n\u001b[0m\u001b[1;32m    137\u001b[0m                           decorator_argspec)\n\u001b[1;32m    138\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorator_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, decorator_name, target, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Certain callables such as builtins can not be inspected for signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3254\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3255\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   3001\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   3003\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_signature_bound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_bound_method\u001b[0;34m(sig)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \"\"\"\n\u001b[1;32m   1963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_VAR_KEYWORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEYWORD_ONLY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = np.asarray(state, dtype=np.float32)\n",
    "done, score = False, 0\n",
    "best_score = -np.inf\n",
    "val_score = -np.inf\n",
    "val_scores = deque(maxlen=50)\n",
    "s = 0\n",
    "s_scores = deque(maxlen=50)\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(512):\n",
    "  state = env.reset()\n",
    "  episodic_reward = 0\n",
    "  s_score = 0\n",
    "  states, next_states, actions, rewards, dones = [], [], [], [], []\n",
    "  ep_reward_list = []\n",
    "  avg_reward_list = []\n",
    "\n",
    "  while True:\n",
    "      tf_prev_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "      action = policy(actor, tf_prev_state, ou_noise)\n",
    "      \n",
    "      next_state, reward, done, info = env.step(action)\n",
    "\n",
    "      next_state = np.asarray(next_state, dtype=np.float32)\n",
    "      states.append(state)\n",
    "      next_states.append(next_state)\n",
    "      actions.append(action)\n",
    "      rewards.append(reward)\n",
    "      dones.append(done)\n",
    "\n",
    "      buffer.record((state, action, reward, state, done))\n",
    "      episodic_reward += reward\n",
    "\n",
    "      buffer.learn()\n",
    "      update_target(target_actor.variables, actor.variables, tau)\n",
    "      update_target(target_critic.variables, critic.variables, tau)\n",
    "\n",
    "      # End this episode when `done` is True\n",
    "      if done:\n",
    "        state, done, score = env.reset(), False, 0\n",
    "        state = np.asarray(state, dtype=np.float32)\n",
    "        break\n",
    "\n",
    "      state = next_state\n",
    "      ep_reward_list.append(episodic_reward)\n",
    "\n",
    "  # Mean of last 40 episodes\n",
    "  avg_reward = np.mean(ep_reward_list[-40:])\n",
    "  print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "  avg_reward_list.append(avg_reward)\n",
    "\n",
    "  a_loss, c_loss = replay(actor, critic, baseline, states, actions, rewards, dones, next_states)\n",
    "\n",
    "# After season\n",
    "success_rate = s_score / sum(dones)\n",
    "s_scores.append(s_score)\n",
    "mean_s_score = np.mean(s_scores)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Coir2_DMAV4o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-yvoJjVAV8_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wG6oe8aXAWBn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
